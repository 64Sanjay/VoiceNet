# Speaker Identification

A modular and extensible PyTorch-based framework for **closed-set speaker identification**. This repository supports state-of-the-art neural architectures (CNN, TDNN/x-vector, ResNet), robust training pipelines, configurable data handling, and evaluation metrics. Designed for research, benchmarking, and practical deployment.

---

## ğŸŒŸ Features

- **Neural architectures:** CNN (incl. 1D), TDNN/x-vector, ResNet
- **Flexible pipelines:** Complete separation of data, models, training, and evaluation
- **Data augmentation:** Noise, channel robustification
- **Configurable:** Plug-and-play experiment configs
- **Easy extensibility:** Add new datasets, models, or metrics with minimal refactoring
- **Research & reproducibility:** GitHub and research-paper friendly structure

---

## ğŸ” Task Pipeline (Conceptual Overview)

```
Audio â†’ Feature Extraction â†’ Speaker Encoder â†’ Classifier â†’ Speaker ID
```
- **Audio**: raw speech data
- **Feature Extraction**: MFCC, FBANK, etc.
- **Speaker Encoder**: Neural network (CNN/TDNN/ResNet)
- **Classifier**: Softmax, AM-Softmax, AAM-Softmax head
- **Speaker ID**: Closed-set speaker classification

---

## ğŸ“ Repository Structure

```
speaker_identification/
â”‚
â”œâ”€â”€ config/                         # Experiment & model configuration
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/                           # Dataset & preprocessing
â”‚   â”œâ”€â”€ aishell/                    # AISHELL speaker ID dataset
â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â”œâ”€â”€ train.txt
â”‚   â”‚   â”œâ”€â”€ train_small.txt
â”‚   â”‚   â”œâ”€â”€ train_full.txt
â”‚   â”‚   â”œâ”€â”€ val.txt
â”‚   â”‚   â”œâ”€â”€ val_small.txt
â”‚   â”‚   â”œâ”€â”€ val_full.txt
â”‚   â”‚   â””â”€â”€ test.txt
â”‚   â”œâ”€â”€ augmentation.py             # Noise & channel augmentation
â”‚   â”œâ”€â”€ preprocessing.py            # Feature extraction (MFCC, FBANK)
â”‚   â”œâ”€â”€ dataset.py                  # Speaker ID dataset loader
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ models/                         # Speaker identification models
â”‚   â”œâ”€â”€ cnn.py                      # CNN / CNN1D models
â”‚   â”œâ”€â”€ tdnn.py                     # TDNN / x-vector models
â”‚   â”œâ”€â”€ resnet.py                   # ResNet-based speaker models
â”‚   â”œâ”€â”€ losses.py                   # Softmax, AM-Softmax, AAM-Softmax
â”‚   â”œâ”€â”€ classifier.py               # Speaker classifier head
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ training/                       # Training pipeline
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ evaluation/                     # Evaluation & metrics
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”œâ”€â”€ metrics.py                  # Accuracy, Top-K
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ outputs/                        # Training outputs
â”‚   â”œâ”€â”€ speaker_id_*/               # Experiment runs
â”‚   â”‚   â”œâ”€â”€ best_model.pt
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_*.pt
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â””â”€â”€ train.log
â”‚   â””â”€â”€ simple_model/
â”‚       â””â”€â”€ model.pt
â”‚
â”œâ”€â”€ utils/                          # Utility functions
â”‚   â”œâ”€â”€ audio_utils.py
â”‚   â”œâ”€â”€ feature_utils.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/                        # Dataset utilities
â”‚   â”œâ”€â”€ download_dataset.py
â”‚   â””â”€â”€ prepare_data.py
â”‚
â”œâ”€â”€ inference.py                    # Speaker ID inference
â”œâ”€â”€ evaluate.py                     # Evaluation entry point
â”œâ”€â”€ train.py                        # Training entry point
â””â”€â”€ __init__.py
```

### âœ… Why this structure is correct

- âœ” Matches speaker identification task (closed-set classification)
- âœ” Supports CNN, TDNN, ResNet, x-vector
- âœ” Clean separation of data â†’ model â†’ training â†’ evaluation
- âœ” Research-paper & GitHub-friendly
- âœ” Easily extensible to multilingual speakers

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/<your-org>/speaker_identification.git
cd speaker_identification
```

### 2. Install Dependencies

*Python 3.8+ recommended*

```bash
pip install -r requirements.txt
```

### 3. Prepare the Dataset

- Download supported datasets (e.g. AISHELL):

```bash
python scripts/download_dataset.py --dataset aishell
```

- Prepare train/val/test splits:

```bash
python scripts/prepare_data.py --dataset aishell
```

### 4. Train a Speaker ID Model

```bash
python train.py --config config/config.py
```

### 5. Evaluate

```bash
python evaluate.py --model outputs/speaker_id_*/best_model.pt --data data/aishell/test.txt
```

### 6. Inference

```bash
python inference.py --model outputs/speaker_id_*/best_model.pt --audio example.wav
```

---

## ğŸ“‘ Configuration

Experiment and model hyperparameters are set in `config/config.py`. Supports easy tweaking of:
- Model type (`cnn`, `tdnn`, `resnet`)
- Feature type (MFCC, FBANK)
- Loss function
- Optimizer, learning rate, batch size

---

## ğŸ“ˆ Evaluation Metrics

- **Accuracy** (Top-1, Top-K)
- **Confusion Matrix**
- **Per-speaker breakdown**

Custom metrics can be added in `evaluation/metrics.py`.

---

## ğŸ› ï¸ Utilities

- **Feature extraction**: `data/preprocessing.py`
- **Augmentation**: `data/augmentation.py`
- **Audio helpers**: `utils/audio_utils.py`

---

## ğŸ¤ How to Contribute

1. Fork this repo, create your branch.
2. Add new models, datasets, or metrics in their respective folders.
3. Submit a pull request with description.

---

## ğŸ“– References

- [x-vector: Robust Speaker Embedding Extraction](https://arxiv.org/abs/1710.10468)
- [ResNet for Speaker Recognition](https://arxiv.org/abs/1908.10234)
- [AISHELL-1 Dataset](https://www.openslr.org/33)
